\documentclass[11pt]{article}
\usepackage{graphicx} % more modern
%\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{epsf}
\usepackage{amsmath,amssymb,amsfonts,verbatim}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
%\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}

\def\A{{\bf A}}
\def\a{{\bf a}}
\def\B{{\bf B}}
\def\b{{\bf b}}
\def\C{{\bf C}}
\def\c{{\bf c}}
\def\D{{\bf D}}
\def\d{{\bf d}}
\def\E{{\bf E}}
\def\e{{\bf e}}
\def\F{{\bf F}}
\def\f{{\bf f}}
\def\G{{\bf G}}
\def\g{{\bf g}}
\def\k{{\bf k}}
\def\K{{\bf K}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\L{{\bf L}}
\def\M{{\bf M}}
\def\m{{\bf m}}
\def\n{{\bf n}}
\def\N{{\bf N}}
\def\BP{{\bf P}}
\def\R{{\bf R}}
\def\BS{{\bf S}}
\def\s{{\bf s}}
\def\t{{\bf t}}
\def\T{{\bf T}}
\def\U{{\bf U}}
\def\u{{\bf u}}
\def\V{{\bf V}}
\def\v{{\bf v}}
\def\W{{\bf W}}
\def\w{{\bf w}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\Q{{\bf Q}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\Z{{\bf Z}}
\def\z{{\bf z}}
\def\0{{\bf 0}}
\def\1{{\bf 1}}


\def\hx{\hat{\bf x}}
\def\tx{\tilde{\bf x}}
\def\ty{\tilde{\bf y}}
\def\tz{\tilde{\bf z}}
\def\hd{\hat{d}}
\def\HD{\hat{\bf D}}

\def\MA{{\mathcal A}}
\def\ML{{\mathcal L}}
\def\MF{{\mathcal F}}
\def\MR{{\mathcal R}}
\def\MG{{\mathcal G}}
\def\MI{{\mathcal I}}
\def\MN{{\mathcal N}}
\def\MO{{\mathcal O}}
\def\MT{{\mathcal T}}
\def\MX{{\mathcal X}}
\def\SW{{\mathcal {SW}}}
\def\MW{{\mathcal W}}
\def\MY{{\mathcal Y}}
\def\BR{{\mathbb R}}
\def\BP{{\mathbb P}}
\def\BE{{\mathbb E}}
\def\BN{{\mathbb N}}

\def\bet{\mbox{\boldmath$\beta$\unboldmath}}
\def\epsi{\mbox{\boldmath$\epsilon$}}

\def\etal{{\em et al.\/}\,}
\def\tr{\mathrm{tr}}
\def\rk{\mathrm{rk}}
\def\diag{\mathrm{diag}}
\def\dg{\mathrm{dg}}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\vecd{\mathrm{vec}}

\def\ph{\mbox{\boldmath$\phi$\unboldmath}}
\def\vp{\mbox{\boldmath$\varphi$\unboldmath}}
\def\pii{\mbox{\boldmath$\pi$\unboldmath}}
\def\Ph{\mbox{\boldmath$\Phi$\unboldmath}}
\def\pss{\mbox{\boldmath$\psi$\unboldmath}}
\def\Ps{\mbox{\boldmath$\Psi$\unboldmath}}
\def\muu{\mbox{\boldmath$\mu$\unboldmath}}
\def\Si{\mbox{\boldmath$\Sigma$\unboldmath}}
\def\lam{\mbox{\boldmath$\lambda$\unboldmath}}
\def\Lam{\mbox{\boldmath$\Lambda$\unboldmath}}
\def\Gam{\mbox{\boldmath$\Gamma$\unboldmath}}
\def\Oma{\mbox{\boldmath$\Omega$\unboldmath}}
\def\De{\mbox{\boldmath$\Delta$\unboldmath}}
\def\de{\mbox{\boldmath$\delta$\unboldmath}}
\def\Tha{\mbox{\boldmath$\Theta$\unboldmath}}
\def\tha{\mbox{\boldmath$\theta$\unboldmath}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]


\def\probin{\mbox{\rotatebox[origin=c]{90}{$\vDash$}}}

\def\calA{{\cal A}}



%this is a comment

%use this as a template only... you may not need the subsections,
%or lists however they are placed in the document to show you how
%do it if needed.


%THINGS TO REMEMBER
%to compile a latex document - latex filename.tex
%to view the document        - xdvi filename.dvi
%to create a ps document     - dvips filename.dvi
%to create a pdf document    - dvipdf filename.dvi
%{\bf TEXT}                  - bold font TEXT
%{\it TEXT}                  - italic TEXT
%$ ... $                     - places ... in math mode on same line
%$$ ... $$                   - places ... in math mode on new line
%more info at www.cs.wm.edu/~mliskov/cs423_fall04/tex.html


\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\notes}[5]{
	\renewcommand{\thepage}{#1 - \arabic{page}}
	\noindent
	\begin{center}
	\framebox{
		\vbox{
		\hbox to 5.78in { { \bf Statistical Machine Learning}
		\hfill #2}
		\vspace{4mm}
		\hbox to 5.78in { {\Large \hfill #5 \hfill} }
		\vspace{2mm}
		\hbox to 5.78in { {\it #3 \hfill #4} }
		}
	}
	\end{center}
	\vspace*{4mm}
}

\newcommand{\ho}[5]{\notes{#1}{Distributions}{Professor: Zhihua Zhang}{}{Lecture Notes #1: Multinomial Distribution}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%begins a LaTeX document
\setcounter{section}{2}
\setcounter{subsection}{4}
\begin{document}
\ho{4}{2014.03.08}{Moses Liskov}{Name}{Lecture title}



\section{Multivariate Random Variables}

\subsection{Bivariate Distribution}
Given a pair of discrete random variable $X$ and $Y$, define the joint mass distribution by $f_{X,Y}(X=x, Y=y) = \BP(X=x, Y=y) = \BP(X=x \text{ and } Y = y)$.

\begin{definition}
In the continuous case, we call a function $f(x,y)$ a probability density function, if
\begin{enumerate}
\item $f(x,y) \geq 0$ for all $x, y$.
\item $\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f(x,y) dxdy = 1$.
\item for any set $A \subset \BR \times \BR$, $\BP((X,Y) \in A) = \iint\limits_A f(x,y) dxdy$. 
\end{enumerate}
\end{definition}
The cumulative distribution function of joint $(X,Y)$ is given by $F_{X,Y} (x, y) = \BP(X \leq x, Y \leq y)$.

\begin{example}
Let $(X,Y)$ have density $f(x,y)= \begin{cases}cx^2y & x^2\le y\le 1 \\ 0 &otherwise\end{cases}$, then
$$\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)dxdy=c\int_{-1}^1\int_{x^2}^1x^2ydydx=c\int_{-1}^1\frac{1}{2}x^2(1-x^2)dx=\frac{4}{21}c=1$$
so $c=\frac{21}{4}$. And $P(X\ge Y)=\frac{21}{4}\int_0^1\int_{x^2}^xx^2ydydx$.
\end{example}

\begin{definition}
If random variable $X$ and $Y$ have joint probability density function $f_{X,Y}(x, y)$, then the marginal distribution function is given by $f_X(x)=P(X=x)=\sum_yP(X=x,Y=y)=\sum_yf_{X,Y}(x,y)$ and $f_Y(y)=P(Y=y)=\sum_xP(X=x,Y=y)=\sum_xf_{X,Y}(x,y)$.
For continous case, $f_X(x) = \int f_{X, Y}(x, y) dy$ and $f_Y(y) = \int f_{X, Y}(x, y) dx$.
\end{definition}

\begin{definition}
Random variables $X$ and $Y$ are independent, if for every $A$ and $B$, $\BP(X\in A, Y\in B) = \BP(X\in A)\BP(Y\in B)$.
\end{definition}

\begin{theorem}
Random variables $X$ and $Y$ have joint probability density function $f_{X,Y}$, then $X$ and $Y$ are independent if and only if $f_{X,Y}(x,y) = f_X(x) f_Y(y)$ for all $x$ and $y$.
\end{theorem}

\begin{definition}
If $f_Y(y) > 0$, then the conditional density function given $Y$ is $f_{X|Y}(x | y) = \BP(X = x| Y = y) = \frac{\BP(X=x, Y= y)}{\BP(Y= y)} = \frac{f_{X,Y}(x, y)}{f_Y(y)}$. And $\BP(X\in A|Y=y)=\int\limits_Af_{X|Y}(x|y)dx$.
\end{definition}

\begin{definition}
Let $X = (X_1, X_2, ..., X_n)$ where $X_i$ is a random variable. We call $X$ a random vector, its probability density function is $f_{X_1, ..., X_n}(x_1, x_2, ..., x_n)$, and the marginal is $f(x_i)= \sum_{x_1, ..., x_{i-1}, x_{i+1}, ..., x_n} f(x_1, ... x_n)$ for discrete case. For continuous case, we will use integral instead.$X_1, X_2, ..., X_n$ are independent if for every $A_i$, $\BP(X_1\in A_1, ... ,X_n\in A_n)=\prod_{i=1}^n\BP(X_i\in A_i)$. Which means that $f(x_1,...,x_n)=\prod_{i=1}^nf_{X_i}(x_i)$.
 
\end{definition}

\begin{definition}
If $X_1,\dots,X_n$ are independent and each has the same marginal distribution with CDF $F$, we say that $X_1,\dots,X_n$ are i.i.d.(independent and identically distributed), $X_1,\dots,X_n\stackrel{\text{i.i.d.}}{\sim}F$.
\end{definition}
\begin{definition}
Let $f(x_1, x_2, ..., x_n)$ be the joint density function of $X_1, X_2, ..., X_n$, $\pi_1, \pi_2, ..., \pi_n$ is a permutation of $\{1, 2, ..., n\}$. If $f(x_1, x_2, ..., x_n) = f(x_{\pi_1}, x_{\pi_2}, ..., x_{\pi_n})$, then $X_1, ..., X_n$ are exchangeable.
\end{definition}

\begin{theorem}(de Finetti)
Let $X_i \subset X$ for all $i\in\{1, 2, ..\}$. Suppose that for any $n$, $x_1, x_2, ..., x_n$ are exchangeable. Then we have \[f(x_1, x_2, ... x_n) = \int \Pi_{i=1}^n f(x_i | \theta) f(\theta) d\theta\] for some parameter $\theta$ with prior distribution $f(\theta)$.
\end{theorem}

\begin{theorem}
If $\theta \sim f(\theta)$ and $X_1, X_2, ..., X_n$ are conditionally iid given $\theta$, then marginally $X_1, X_2, ... X_n$ are exchangeable.
\end{theorem}
\begin{proof}
\[
\begin{aligned}
f(x_{\pi_1}, x_{\pi_2}, ..., x_{\pi_n})&=\int f(x_{\pi_1}, x_{\pi_2}, ..., x_{\pi_n}|\theta)f(\theta)d\theta \\
&=\int\prod_{i=1}^nf(x_{\pi_i}|\theta)f(\theta)d\theta=\int\prod_{i=1}^nf(x_i|\theta)f(\theta)d\theta=f(x_1, x_2, ... x_n)
\end{aligned}
\]
\end{proof}
\subsection{Expectations and Moments}
\begin{definition}
The mean of a random variable X is $E(x)=\int xdF(x)=\begin{cases}\sum\limits_xxf(x)  & \text{x is discrete} \\ \int xf(x)dx & \text{x is continous} \end{cases}$. Note that $E(x)$ exists if $\int|x|dF(x)<\infty$.
\end{definition}
\begin{example}
$Ga(x|\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$, $x>0$, then 
$E(x)=\int_0^{\infty}x\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}dx=\int_0^{\infty}\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha}e^{-\beta x}dx=\frac{\Gamma(\alpha+1)}{\beta^{\alpha+1}}\frac{\beta^{\alpha}}{\Gamma(\alpha)}\int_0^{\infty}x^{\alpha+1-1}e^{-\beta x}\frac{\beta^{\alpha+1}}{\Gamma(\alpha+1)}dx=\frac{\alpha}{\beta}$. While, from another point of view, $\int x^{\alpha-1}e^{-\beta x}dx=\frac{\Gamma(\alpha)}{\beta^{\alpha}}$, and $\int xx^{\alpha-1}e^{-\beta x}dx=\frac{\Gamma(\alpha)\alpha}{\beta^{\alpha+1}}$, so $\int x\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}dx=\frac{\alpha}{\beta}$.
\end{example}
\begin{example}
$f(x)=\frac{\pi}{1+x^2}$, then $\int_{-\infty}^{+\infty}\pi \frac{|x|}{1+x^2}dx=2\pi\int_0^{+\infty}\frac{x}{1+x^2}dx=\pi log(1+x)|_0^{+\infty}=\infty$, so $E(x)$ doesn't exist.
\end{example}
\begin{definition}
Let $Y=g(X)$, then $E(Y)=E(g(X))=\int g(x)dF_X(x)$. If $Z=g(X,Y)$, then $E(Z)=E(g(X,Y))=\iint g(x,y)dF_{X,Y}(x,y)$.
\end{definition}
\begin{definition}
The mean of $k^{th}$ moment of $X$ is $E(X^k)=\int x^kdF(x)$, assuming $E(X^k) exists$. 
\end{definition}
\begin{theorem}
If the $k^{th}$ moment exists, then the $j^{th}$ moment for $j<k$ exists.
\begin{proof}
$E(|x|)=\int_{-\infty}^{+\infty}|x|^jdF(x)=\int\limits_{|x|\le 1}|x|^jdF(x)+\int\limits_{|x|>1}|x|^jdF(x)\le\int\limits_{|x|\le 1}dF(x)+\int\limits_{|x|>1}|x|^kdF(x)\le\int\limits_RdF(x)+\int\limits_R|x|^kdF(x)=1+E(|x|^k)<\infty$.
\end{proof}
\end{theorem}
\begin{theorem}
If $X_1,\dots,X_n$ are random variables, and $a_1,\dots,a_n$ are constants, then $E(\sum_{i=1}^na_iX_i)=\sum_{i=1}^nE(X_i)$.
\end{theorem}
\begin{theorem}
If $X_1,\dots,X_n$ are independent random variables, then $E(\prod_{i=1}^nX_i)=\prod_{i=1}^nE(X_i)$.
\end{theorem}
\subsection{Variance and Convariance}
The variance is $\sigma^2=E((x-\mu)^2)=Var(X)=\int(x-\mu)^2dF(x)$, the standard deriation(std) is $std(X)=\sqrt{Var(X)}$.
\begin{enumerate}
\item $Var(X)=E(X^2)-\mu^2$.
\item If $a$ and $b$ are constants, then $Var(aX+b)=a^2Var(X)$.
\item If $X_1,\dots,X_n$ are independent and $a_1,\dots,a_n$ are constants, then $Var(\sum_{i=1}^na_iX_i)=\sum_{i=1}^na^2Var(X_i)$.
\end{enumerate}
Sample mean is $\overline{x}_n=\frac{1}{n}\sum_{i=1}^nx_i$, and the sample variance is $s_n^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\overline{x}_n)^2$.
\begin{theorem}
If $X_1,\dots,X_n$ are independent random variables, and $\mu=E(X_i)$, $\sigma^2=Var(X_i)$. Then $E(\overline{X}_n)=\mu$, $Var(\overline{X}_n)=\frac{\sigma^2}{n}$ and $E(S_n^2)=\sigma^2$.
\end{theorem}
\begin{example}
If $X\sim Binomial(n,q)$, then $E(X)=\sum_{x=0}^nx{n\choose x}q^x(1-q)^{n-x}=\sum_{x=1}^nx{n\choose x}q^x(1-q)^{n-x}=\sum_{x=1}^nn{n-1\choose x-1}q^x(1-q)^{n-x}=\sum_{y=0}^{n-1}n{n-1\choose y}q^{y+1}(1-q)^{n-y-1}=nq$, $Var(X)=\sum_{x=0}^nx^2{n\choose x}q^x(1-q)^{n-x}-n^2q^2=nq(1-q)$.
\end{example}
\begin{definition}
$X$ and $Y$ are random variables with $\mu_X$, $\mu_Y$, $\sigma_X$ and $\sigma_Y$, then 
$$Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))$$the correlation $$\rho=\rho_{X,Y}=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$
\begin{enumerate}
\item $Cov(X,Y)=E(XY)-E(X)E(Y)$.
\item $-1\le\rho(X,Y)\le 1$.
\item If $Y=aX+b$, $a$, $b$ are constants, then $\rho(X,Y)=1$ if $a>0$ and $\rho(X,Y)=-1$ if $a<0$.
\item If $X$ and $Y$ are independent, then $Cov(X,Y)=\rho=0$. Note that the converse is not true in general.
\item $$Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$$ $$Var(X-Y)=Var(X)+Var(Y)-2Cov(X,Y)$$ generally, $$Var(\sum_{i=1}^na_iX_i)=\sum_{i=1}^na_i^2Var(X_i)+2\sum\limits_{i<j}a_ia_jCov(X_i,X_j)$$
\end{enumerate}
\end{definition}
\begin{definition}
	Let random vector $\X=(X_1,\dots,X_n)^T$. Then mean of $\X$ is 
		$$\mu=(\mu_1,\dots,\mu_n)^T=(\BE[X_1],\dots,\BE[X_n])^T.$$
	The covariance matrix $\Si$ is
		$$\Si = Var(\X) = 
			\begin{pmatrix}
				Var(X_1)		& Cov(X_1,X_2)	& \cdots 	& Cov(X_1,X_n)\\
				Cov(X_1,X_2)	& Var(X_2)	 	& \cdots 	& Cov(X_2,X_n)\\	
				\vdots 		& \vdots 		& \ddots 	& \vdots\\
				Cov(X_n,X_1) 	& Cov(X_n,X_2)& \cdots 	& Var(X_n,X_n)
			\end{pmatrix}.
		$$
\end{definition}
\begin{theorem}
	If $\a$ is a vector and $\X$ is a random vector with mean $\muu$ and covariance matrix $\Si$, then
		$$\BE[\a^T\X]=\a^T\muu \quad \text{and} \quad Var(\a^T\X)=\a^T\Si\a$$.
	If $\A$ is a matrix then
		$$\BE[\A\X] = \A\muu  \quad \text{and} \quad Var(\A\X) = \A\Si\A^T$$.
\end{theorem}

\subsection{Conditional Expectation}
$\BE(X)$ is a number, $\BE(X|Y=y)$ is a function of $y$, and $\BE(X|Y)$ is a random variable whose value is $\BE(X|Y=y)$.
$$\BE(X|Y=y)=\begin{cases}\sum\limits_xxf_{X|Y}(x|y)  & \text{x is discrete} \\ \int xf_{X|Y}(x|y)dy & \text{x is continous} \end{cases}$$
$$\BE(g(X,Y)|Y=y)=\begin{cases}\sum\limits_xg(x,y)f_{X|Y}(x|y)  & \text{x is discrete} \\ \int g(x,y)f_{X|Y}(x|y)dy & \text{x is continous} \end{cases}$$
\begin{example}
Suppose we draw $Y\sim Unif(0,1)$. After we observe $Y=y$, we draw $[X|Y=y]\sim Unif(y,1)$.
$$f_{X|Y}(x|y)=\frac{1}{1-y}, (y<x<1)$$
$$\BE(X|Y=y)=\int_y^1\frac{x}{1-y}dx=\frac{1+y}{2}$$
$$\BE(X|Y)=\frac{1+Y}{2}$$
\end{example}
\begin{theorem}
[The rule of iterated expectation] For $X$ and $Y$, assuming the expectations exist, we have $\BE(\BE(Y|X))=\BE(Y)$ and $\BE(\BE(X|Y))=\BE(X)$.Generally, $$\BE(\BE(g(X,Y)|X))=\BE(g(X,Y))=\int g(x,y)dF(x,y)$$
\begin{proof}
$$\BE(\BE(Y|X))=\int\BE(Y|X=x)f_X(x)dx=\iint yf_{Y|X}(y|x)f_X(x)dxdy=\iint yf_{X,Y}(x,y)dxdy=\BE(Y)$$
\end{proof}
\end{theorem}
\begin{definition}
$Var(Y|X=x)=\int(y-\hat{\mu}(x))^2f(y|x)dy$, where $\hat{\mu}(x)=\BE(Y|X=x)$.
\end{definition}
\begin{theorem}
$Var(Y)=\BE(Var(Y|X))+Var(\BE(Y|X))$, so $Var(Y)\ge Var(\BE(Y|X))$.
\end{theorem}
\begin{example}
Draw a document at random from the web, then draw n words at random from the document. Let $X$ be the number of those words who have a certain string. If $Q$ denotes the proportion of words in that document with the string, then $Q$ is also a random variable because it varies from document to document. 

Given $Q=q$, we have that $X\sim Binomial(n,q)$, suppose $Q\sim Uniform(0,1)$. Then
$$\BE(X|Q=q)=nq, Var(X|Q=q)=nq(1-q)$$
$$Var(X)=\BE Var(X|Q)+Var\BE(X|Q)=n\BE(Q(1-Q))+nVar(Q)$$
\end{example}
\subsection{Transformation}
Random variable $X$ has pdf $f_X$ and cmf $F_X$. Let $Y = g(X)$ be a function of $X$. In the discrete case, the pmf of $Y$ is $f_Y(y) = \BP(Y = y) = \BP(g(X) = y) = \BP(x \in g^{-1}(y))$.

\begin{example}
Suppose $\BP(X=-1) = \BP(X=1) = \frac{1}{4}$ and $\BP(X=0) = \frac{1}{2}$. Let $Y=X^2$. So $\BP(Y=0) = \frac{1}{2}$, $\BP(Y=1)=\frac{1}{2}$.
\end{example}

In the continuous case, the steps to find density of transformation variable is given by:
\begin{enumerate}
\item For each y, find set $A_y = \{x:g(x) \leq y\}$.
\item Find CDF, $F_Y(y) = \BP(Y \leq y) = \BP(g(x) \leq y) = \BP(\{x: g(x)\leq y\}) = \int_{A_y} f_X(x) dx$.
\item $f_Y(y) = F_Y'(y)$.
\end{enumerate}

\begin{example}
  $f_X(x) = e^{-x}$ for $x>0$, and $Y=g(X)=\log X$. Then $F_X(x) = \int_0^x f_X(u) du = 1 - e^{-x}$. $A_Y = \{x: x \leq e^y\}$. $F_Y(y) = \BP(Y \leq y) = \BP(\log x \leq y) = \BP(x \leq e^y) = F_X(e^y) = 1 - e^{-e^y}$. $f_Y(y) = (1 - e^{-e^y})' = e^y e^{-e^y}$.
\end{example}

\begin{example}
  $X \sim Uniform(-1, 3)$, $Y=X^2$. $f_X(x) = \begin{cases}\frac{1}{4} & x\in(-1,3) \\ 0 & \text{o.w.}\end{cases}$. Now let us think about the distribution density of $Y$. $Y$ can take value in $(0, 9)$. 
  \begin{enumerate}
    \item $0 < Y < 1$. $A_y = \{X : X^2 \leq y\} = [-\sqrt{y}, \sqrt{y}]$. $F_Y(y) = \int_{A_y} f_X(x) dx = \frac{1}{2} \sqrt{y}$.
    \item $1 \leq Y < 9$. $A_y = [-1, -\sqrt{y}]$. $F_Y(y) = \int_{A_y} \frac{1}{4} dx = \frac{1}{4}(1 + \sqrt{y})$. 
  \end{enumerate}
  So, $f_Y(y) = \begin{cases} \frac{1}{4\sqrt{y}}  & 0 < y < 1 \\ \frac{1}{8\sqrt{y}}  & 1 \leq y < 9\end{cases}$
\end{example}

If random variable $Z = g(X, Y)$, then the way to find density of $Z$ is given by:
\begin{enumerate}
\item For each $z$, find $A_z = \{(x, y): g(x, y)\leq z\}$.
\item Find CDF $F_Z(z) = \BP(Z \leq z) = \iint\limits_{A_z} f_{X,Y}(x, y) dxdy$.
\item $f_Z(z) = F_Z'(z)$.
\end{enumerate}

\begin{example}
  Let $X_1, X_2 \stackrel{iid}{\sim} Uniform(0, 1)$, $Y = X_1 + X_2$. $f_{X_1, X_2}(x_1, x_2) = \begin{cases} 1 & 0<x_1 < 1, 0 < x_2 < 1 \\ 0 & \text{o.w.} \end{cases}$. $F_Y(y) = \BP(\{(x_1, x_2): (x_1 + x_2) \leq y \}) = \iint\limits_{A_y} f(x_1, x_2) dx_1dx_2 = \begin{cases}\frac{1}{2}y^2 & 0<y<1 \\
  1 - \frac{(1-y)^2}{2} & 1 \leq y \leq 2 \\ 1 & y > 2 \\ 0 & y \leq 0 \end{cases}$. So,
  $f_Y(y) = \begin{cases}y & 0\leq y \leq 1 \\ 1 - y & 1 < y \leq 2 \\ 0 & \text{o.w.} \end{cases}$
\end{example}

\begin{theorem}
Let $X$ have CDF $F_X(x)$ and $Y=g(X)$, and let $\mathcal{X} = \{x : f_X(x) > 0\}$, $\mathcal{Y} = \{y: y = g(x) \text{ for some }x \in $X$\}$
\begin{enumerate}
\item if $g$ is a strictly incresing function on $\mathcal{X}$, $F_Y(g) = F_X(g^{-1}(y))$ for $y \in \mathcal{Y}$.
\item if $g$ is a strictly decreasing function on $\mathcal{X}$ and $X$ is a continuous  random variable. $F_Y(y) = 1 - F_X(g^{-1}(y))$ for $y\in\mathcal{Y}$
\end{enumerate}
\end{theorem}

\begin{theorem}
Let $X$ have continuous pdf $f_X(x)$, $Y=g(X)$, and $g$ is strictly monotone function, then $f_Y(y) = f_X(g^{-1}(y)) |\frac{d}{dy}g^{-1}(y)| $ 
\end{theorem}

\begin{proof}
According to two case in theorem 3.4. 
\begin{enumerate}
\item $g$ is a strictly increasing function on $\mathcal{X}$, then 
$f_Y(y) = \frac{dF_Y(y)}{dy} = f_X(g^{-1}(y)) \frac{dg^{-1}(y)}{dy}$
\item $g$ is a strictly decreasing function on $\mathcal{X}$, then 
$f_Y(y) = \frac{dF_Y{y}}{dy} =-f_X(g^{-1}(y))\frac{dg^{-1}(y)}{dy}$.
\end{enumerate}
So, we can combine them to $f_Y(y) = f_X(g^{-1}(y)) |\frac{dg^{-1}(y)}{dy}|$.
\end{proof}

\begin{theorem}(Probability integral transformation)
Let $X$ has a continuous cdf $F_X(x)$, $Y=F_X(x)$. Then $Y$ has uniform distribution on $(0, 1)$, i.e. $\BP(Y \leq y) = y$ where $0 \leq y \leq 1$.
\end{theorem}

\begin{proof}
$\BP(Y \leq y) = \BP(F_X(x) \leq y) = \BP(F_X^{-1}(F_X(x)) \leq F_X^{-1}(y)) = \BP(x \leq F_X^{-1}(y)) = F_X(F_X^{-1}(y)) = y$.
\end{proof}

\subsection{Moments Generating Function(MGF)}
\begin{definition}
  In probability theory and statistics, the moment-generating function of a random variable $X$ is 
  \[M_X(t) = \BE[e^{tX}] = \int e^{tx} f_X(x) dx\]
\end{definition}
One property about moment-generating function is that we can get $\BE[X^k]$ from $M_X^{(k)}(0)$, 
as we can see $M_X^{(k)}(t) = \int x^k e^{tx} f_X(x) dx$, where we assume we can put the derivation inside. So $M_X^{(k)}(0) = \BE[X^k]$.
\begin{definition}
Laplace transformation $\ML(t)=\int e^{-tx}dF(x)$.\
\begin{enumerate}
\item If $Y=aX+b$, then $M_Y(t)=e^{bt}M_X(at)$
\item If $X_1,\dots,X_n$ are independent and $Y=\sum\limits_iX_i$, then $M_Y(t)=\prod\limits_iM_{X_i}(t)$.
\item Let $X$ and $Y$ be random variables, if $M_X(t)=M_Y(t)$ for all $t$ in an open integral around 0, then denote $X
\stackrel{d}{=}Y$.
\item $\ML(\mu)=\int e^{-\mu x}d\mu(x)$, $x,\mu\ge 0$ and $\mu(x)$ is non-decreasing and the integral converges for $\mu \in (0,+\infty)$. Then $\ML^{'}(\mu)=\int_0^{\infty}-e^{-\mu x}xd\mu(x)$, and $\ML^{(k)}(\mu)=\int_0^{\infty}(-1)^ke^{-\mu x}x^kd\mu(x)$. So $(-1)^k\ML^{(k)}(\mu)=\int_0^{\infty}e^{-\mu x}x^kd\mu(x)\ge 0$.
\end{enumerate} 
\end{definition}
\begin{definition}
  A function $f:(0, \infty) \rightarrow \BR$ is completely monotone function if and only if $f$ is of class $C^\infty$(infinitely derivable), and $(-1)^n f^{(n)}(\lambda) \geq 0$ for all $n \in N\cup\{0\}$, and $\lambda > 0$.
\end{definition}

\begin{theorem}(Bernstein)
  Let $g:(0,\infty) \rightarrow \BR$ be a completely monotone function. Then it is the Laplace transform of an unique measure $\mu$ on $[0,\infty]$, i.e. for all $\lambda > 0$, \[g(\lambda) =  \ML(\mu; \lambda) = \int_{[0, \infty)} e^{-\lambda t} \mu(dt)\] Conversely, whenever $\ML(\mu; \lambda) < \infty$ for every $\lambda > 0$,  $\lambda \mapsto  \ML(\mu; \lambda)$ is a completely monotone function. Furthermore, $\mu(x)$ is a probability distribution iff $g(0)=1$.
\end{theorem}

\begin{proof}
  Assume $g(0+) = 1$ and $g(+\infty) = 0$. By Taylor's formula
  \begin{equation}\begin{split} 
    f(\lambda) &= \sum_{k=0}^{n-1} \frac{f^{(k)}(a)}{k!} (\lambda - a)^k + \int_a^\lambda \frac{f^{(n)}(s)}{(n-1)!}(\lambda - s)^{n-1} ds \\
    &= \sum_{k=0}^{n-1} \frac{(-1)^k f^{(k)}(a)}{k!} (a-\lambda)^k + \int_\lambda^a \frac{(-1)^nf^{(n)}(s)}{(n-1)!} (s-\lambda)^{n-1} ds  
  \end{split}\end{equation}
  
  where $a > 0$ and $n \in \BN$. Let $a \to \infty$, then 
  \[\begin{split}
    \lim_{a\to\infty} \int_\lambda^a \frac{(-1)^n f^{(n)}(s)}{(n-1)!} (s-\lambda)^{n-1} ds &= 
    \int_\lambda^\infty \frac{(-1)^n f^{(n)}(s)}{(n-1)!} (s-\lambda)^{n-1} ds \\
    &\leq f(\lambda).
  \end{split}\]
  So the sum in (1) converges for every $n \in \BN$ as $a \to \infty$. Let 
  \[ \rho_n(\lambda) = \lim_{a \to \infty} \frac{(-1)^n f^{(n)}(a)}{n!} (a-k)^n\].
  This limit doesn't depend on $\lambda > 0$. Indeed, for $k>0$,
  \[\begin{split}
    \rho_n(k) &= \lim_{a \to \infty} \frac{(-1)^n f^{(n)}(a)}{n!} (a - k)^n  \\
    &= \lim_{a \to \infty} \frac{(-1)^n f^{(n)}(a)}{n!} (a-\lambda)^n \frac{(a -k)^n}{(a-\lambda)^n} \\
    &= \rho_n(\lambda).
  \end{split}\]
  So we can get 
  \[ f(\lambda) = \sum_{k=0}^{n-1} \rho_k(\lambda) + \int_\lambda^\infty \frac{(-1)^n f^{(n)}(s)}{(n-1)!} (s-\lambda)^{n-1} ds\]
  Let $\lambda \to \infty$, since $f(+\infty) = 0$, so $\rho_k(\lambda) = 0$. Then we can get 
  \begin{equation} f(\lambda) = \int_\lambda^\infty \frac{(-1)^nf^{(n)}(s)}{(n-1)!}(s-\lambda)^{n-1} ds\end{equation}. 
  And since $f(0+) = 1$, we can get:
  \[1 = \lim_{\lambda \to 0+} f(\lambda) = \int_0^\infty \frac{(-1)^nf^{(n)}(s)}{(n-1)!} s^{n-1} ds\]
  And (2) can also be written as:
  \[ f(\lambda) = \int_0^\infty (1-\frac{\lambda}{s})_+^{n-1} \frac{(-1)^nf^{(n)}(s)}{(n-1)!} s^{n-1}ds. \]
 Let $t = \frac{n}{s}$, then 
 \[ f(\lambda) = \int_0^\infty (1-\frac{\lambda t}{n})^{n-1}_+ \frac{(-1)^n}{n!}f^{(n)}(\frac{n}{t}) (\frac{n}{t})^{n+1} dt\].
 Since $\lim_{n\to\infty} (1-\frac{\lambda t}{n})^{n-1}_+ = e^{-\lambda t}$. So
 \[ f(\lambda) = \int_0^\infty e^{-\lambda t} \frac{(-1)^n}{n!}f^{(n)}(\frac{n}{t}) (\frac{n}{t})^{n+1} dt.\]
 
 
 For the converse, let $f(\lambda) = \ML(\mu; \lambda) = \int_0^\infty e^{-\lambda t} \mu(dt)$. So
 \[\begin{split}(-1)^n f^{(n)}(\lambda) &= \int_0^\infty t^n e^{-\lambda t} \mu(dt) \geq 0  \end{split}\]

\end{proof}

\textbf{Corollary}
Let $g(t)$ be a function that is symmetric about the origin, integrable, convex and twice differentible on $(0, \infty)$ and $g(0^+) = 1$, $g(+\infty) = 0$ then
\[ g(t) = \int_0^{\infty}(1-\frac{t}{s})_+ s g''(s) ds \]

\begin{theorem}
A density function $f(x)$(symmetric about $0$) can be represented as a Gaussian scale mixture iff $f(\sqrt{x})$ is completely monotone on $(0, \infty)$.
\end{theorem}

\begin{proof}
\[\begin{split}
Let~g(x) = f(\sqrt{x}).\\ 
& f(\sqrt{x})~is~completely~monotone,\\ 
\Longleftrightarrow & g(x)~is~completely~monotone.\\
By~Bernstein:\\
\Longleftrightarrow & g(x) = \int_{0}^{\infty}e^{-xt}\mu(\mathrm{d}t)\\
\Longleftrightarrow & f(\sqrt{x}) = \int_{0}^{\infty}e^{-xt}\mu(\mathrm{d}t)\\
\Longleftrightarrow & f(x) = \int_{0}^{\infty}e^{-x^2t}\mu(\mathrm{d}t) = C\int_{0}^{\infty}N(x\mid0, \frac{1}{2t})\mu(\mathrm{d}t),
~~and
\int_{0}^{\infty}\mu(dt) = 1\\
\Longleftrightarrow & f(x)~can~be~represented~as~a~Gaussian~scale~mixture.
\end{split}\]
\end{proof}

\begin{theorem}
If $f(x) > 0$, then $e^{-uf(x)}$ is completely monotone for every $u>0$ iff $f'(x)$ is completely monotone.
\end{theorem}


\begin{proof}
If $e^{-uf(x)}$ is completely monotone for every $u>0$:
\[e^{-\mu f(x)} = \sum_{j=0}^{\infty}\frac{(-1)^j\mu^j}{j!}[f(x)]^j\] and all of its formal derivatives converge uniformly, so we can calculate $\frac{d^n}{dx^n}e^{-\mu f(x)}$ by termwise differentiation.
Since $e^{-\mu f}$ is completely monotone, we have:
\[0\le(-1)^n\frac{d^n}{dx^n}e^{-\mu f(x)} = \sum_{j=1}^{\infty}\frac{\mu^j}{j!}(-1)^{n+j}\frac{d^n}{dx^n}[f(x)]^j\]
As $\mu > 0$, dividing $\mu$, there is:
\[0\le(-1)^{n+1}\frac{d^n}{dx^n}f(x)+\sum_{j=2}^{\infty}\frac{\mu^{j-1}}{j!}(-1)^{n+j}\frac{d^n}{dx^n}[f(x)]^j\]
Then let $\mu \rightarrow 0$:
\[0\le(-1)^{n-1}\frac{d^{n-1}}{dx^{n-1}}f'(x)\]
Eventually, $f'(x)$ is completely monotone.

If $f'(x)$ is completely monotone:
\[(-1)^{n-1}\frac{d^n}{dx^n}f(x)\ge0\]
Let $g(\lambda) = e^{-\lambda},~ \lambda = f(x)$:
\[h(x) = e^{- f(x)} = g(\lambda)\circ f(x)\]
And there is a formula for the n-th derivative of the composition $h = g\circ f$:
\[h^{(n)}(\lambda) = \sum_{(m,i_1,...,i_l)}^{}\frac{n!}{i_1!...i_l!}g^{(m)}(f(\lambda))\prod_{j=1}^{l}(\frac{f^{(j)}(\lambda)}{j!})^{i_j},\]
where $\sum_{j=1}^{l}j\cdot i_j = n$ and $\sum_{j=1}^{l}i_j = m$.

We can see that $n = m + \sum_{j=1}^{l}(j-1)\cdot i_j$.

We have $(-1)^mg^{(m)}(f(x)) \ge 0$ and $(-1)^{j-1}f^{(j)}\lambda \ge 0$. 

So $(-1)^nh^{(n)}(x) \ge 0$ which means $e^{-f(x)}$ is completely monotone.

And $e^{-\mu f(x)}$ is completely monotone.


\end{proof}
\end{document}




