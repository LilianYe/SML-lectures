\documentclass[11pt]{article}
\usepackage{graphicx} % more modern
%\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{epsf}
\usepackage{amsmath,amssymb,amsfonts,verbatim}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
%\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}

\def\A{{\bf A}}
\def\a{{\bf a}}
\def\B{{\bf B}}
\def\b{{\bf b}}
\def\C{{\bf C}}
\def\c{{\bf c}}
\def\D{{\bf D}}
\def\d{{\bf d}}
\def\E{{\bf E}}
\def\e{{\bf e}}
\def\F{{\bf F}}
\def\f{{\bf f}}
\def\G{{\bf G}}
\def\g{{\bf g}}
\def\k{{\bf k}}
\def\K{{\bf K}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\L{{\bf L}}
\def\M{{\bf M}}
\def\m{{\bf m}}
\def\n{{\bf n}}
\def\N{{\bf N}}
\def\BP{{\bf P}}
\def\R{{\bf R}}
\def\BS{{\bf S}}
\def\s{{\bf s}}
\def\t{{\bf t}}
\def\T{{\bf T}}
\def\U{{\bf U}}
\def\u{{\bf u}}
\def\V{{\bf V}}
\def\v{{\bf v}}
\def\W{{\bf W}}
\def\w{{\bf w}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\Q{{\bf Q}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\Z{{\bf Z}}
\def\z{{\bf z}}
\def\0{{\bf 0}}
\def\1{{\bf 1}}


\def\hx{\hat{\bf x}}
\def\tx{\tilde{\bf x}}
\def\ty{\tilde{\bf y}}
\def\tz{\tilde{\bf z}}
\def\hd{\hat{d}}
\def\HD{\hat{\bf D}}

\def\MA{{\mathcal A}}
\def\MF{{\mathcal F}}
\def\MR{{\mathcal R}}
\def\MG{{\mathcal G}}
\def\MI{{\mathcal I}}
\def\MN{{\mathcal N}}
\def\MO{{\mathcal O}}
\def\MT{{\mathcal T}}
\def\MX{{\mathcal X}}
\def\SW{{\mathcal {SW}}}
\def\MW{{\mathcal W}}
\def\MY{{\mathcal Y}}
\def\BR{{\mathbb R}}
\def\BP{{\mathbb P}}
\def\BE{{\mathbb E}}

\def\bet{\mbox{\boldmath$\beta$\unboldmath}}
\def\epsi{\mbox{\boldmath$\epsilon$}}

\def\etal{{\em et al.\/}\,}
\def\tr{\mathrm{tr}}
\def\rk{\mathrm{rk}}
\def\diag{\mathrm{diag}}
\def\dg{\mathrm{dg}}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\vecd{\mathrm{vec}}

\def\ph{\mbox{\boldmath$\phi$\unboldmath}}
\def\vp{\mbox{\boldmath$\varphi$\unboldmath}}
\def\pii{\mbox{\boldmath$\pi$\unboldmath}}
\def\Ph{\mbox{\boldmath$\Phi$\unboldmath}}
\def\pss{\mbox{\boldmath$\psi$\unboldmath}}
\def\Ps{\mbox{\boldmath$\Psi$\unboldmath}}
\def\muu{\mbox{\boldmath$\mu$\unboldmath}}
\def\Si{\mbox{\boldmath$\Sigma$\unboldmath}}
\def\lam{\mbox{\boldmath$\lambda$\unboldmath}}
\def\Lam{\mbox{\boldmath$\Lambda$\unboldmath}}
\def\Gam{\mbox{\boldmath$\Gamma$\unboldmath}}
\def\Oma{\mbox{\boldmath$\Omega$\unboldmath}}
\def\De{\mbox{\boldmath$\Delta$\unboldmath}}
\def\de{\mbox{\boldmath$\delta$\unboldmath}}
\def\Tha{\mbox{\boldmath$\Theta$\unboldmath}}
\def\tha{\mbox{\boldmath$\theta$\unboldmath}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]


\def\probin{\mbox{\rotatebox[origin=c]{90}{$\vDash$}}}

\def\calA{{\cal A}}



%this is a comment

%use this as a template only... you may not need the subsections,
%or lists however they are placed in the document to show you how
%do it if needed.


%THINGS TO REMEMBER
%to compile a latex document - latex filename.tex
%to view the document        - xdvi filename.dvi
%to create a ps document     - dvips filename.dvi
%to create a pdf document    - dvipdf filename.dvi
%{\bf TEXT}                  - bold font TEXT
%{\it TEXT}                  - italic TEXT
%$ ... $                     - places ... in math mode on same line
%$$ ... $$                   - places ... in math mode on new line
%more info at www.cs.wm.edu/~mliskov/cs423_fall04/tex.html


\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\notes}[5]{
	\renewcommand{\thepage}{#1 - \arabic{page}}
	\noindent
	\begin{center}
	\framebox{
		\vbox{
		\hbox to 5.78in { { \bf Statistical Machine Learning}
		\hfill #2}
		\vspace{4mm}
		\hbox to 5.78in { {\Large \hfill #5 \hfill} }
		\vspace{2mm}
		\hbox to 5.78in { {\it #3 \hfill #4} }
		}
	}
	\end{center}
	\vspace*{4mm}
}

\newcommand{\ho}[5]{\notes{#1}{Distributions}{Professor: Zhihua Zhang}{}{Lecture Notes #1: Scale Mixture Distribution}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%begins a LaTeX document
\setcounter{section}{3}
\setcounter{subsection}{0}
\begin{document}
\ho{3}{2014.03.08}{Moses Liskov}{Name}{Lecture title}
\subsection{Scale Mixture Distribution}
We will show several distributions can be seen as the scale mixture of distribution, which is defined as follows,
\[\begin{split}
  X \sim F(\theta) \\
  \theta \sim G(\lambda)
\end{split}\],
So, $T(x) = \int_{\theta} F(\theta) G(\lambda) d\theta$ can be seen as a scale mixture of $F$, where the scale has distribution $G$.

\subsubsection{Student's t-distribution}
The Student's t-distribution is a scale of Gaussian distribution, where the scale has a Gamma distribution. 
Let $X \sim N(\mu, \frac{\sigma^2}{r})$, $r \sim Gamma(\frac{\nu}{2},\frac{\nu}{2})$, then the integral will be:
\[\begin{split}  &  \int_0^{\infty} \frac{r^{-1/2}}{\sqrt{2\pi}\sigma} e^{-\frac{r(x-\mu)^2}{2\sigma^2}}  
\frac{(\frac{\nu}{2})^{\frac{\nu}{2}}}{\Gamma{(\frac{\nu}{2}})} r^{\frac{\nu}{2}-1} e^{-\frac{\nu}{2}r} dr \\
= & \frac{(\frac{\nu}{2})^{\frac{\nu}{2}}}{\Gamma{(\frac{\nu}{2})}\sigma\sqrt{2\pi}}
\int_0^\infty r^{\frac{\nu}{2} - \frac{3}{2}} 
e^{-\frac{r}{2}(\frac{(x-\mu)^2}{\sigma^2} + \frac{\nu}{2})}  dr \\
= & \frac{\nu^{\frac{\nu}{2}}\Gamma{(\frac{\nu+1}{2})}}{\sigma\sqrt{\pi}\Gamma{(\frac{\nu}{2})}} 
\left[\frac{(x-\mu)^2}{\sigma^2} + \frac{\nu}{2}\right]^{\frac{\nu+1}{2}}
\end{split}\]

Note that during the integral, we use a math trick. Since we know $\int_0^\infty \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} dx = 1$ from Gamma distribution, so we can get 
$\int_0^\infty x^{\alpha - 1} e^{-\beta x} dx = \frac{\Gamma(\alpha)}{\beta^\alpha}$. 
This trick will be often used in the follows.

\subsubsection{Laplace Distribution}
The Laplace distribution is a scale of Gaussian distribution, where the scale has a exponential distribution.
Let $X \sim N(\mu, r)$, $r \sim Exponential(\frac{1}{2\sigma^2})$, then we can get the mixture distribution:
\[\begin{split} &
\int_0^\infty \frac{1}{\sqrt{2\pi r}} e^{-\frac{(x-\mu)^2}{2r}}
\frac{1}{2\sigma^2} e^{-\frac{r}{2\sigma^2}} dr  \\
 = & \frac{1}{2\sigma^2 \sqrt{2\pi}}  \int_0^\infty
 r^{\frac{1}{2} - 1} e^{-\frac{1}{2} \left( \frac{(x-\mu)^2}{r} + \frac{r}{\sigma^2}\right)} dr  \\
 = &\frac{1}{2\sigma} e^{\frac{|x-\mu|}{\sigma}}
\end{split}\]

\subsubsection{Negative Binomial Distribution}
Negative Binomial Distribution is a scale of Poisson distribution, where the scale has a Gamma distribution.
Let $K \sim Poisson(\lambda)$, $\lambda \sim Gamma(r, \frac{1-p}{p})$, then we can get the mixture distribution:
\[\begin{split}
&  \int_0^\infty \frac{\lambda^k}{k!} e^{-\lambda}
\frac{\lambda^{r-1} e^{-\frac{1-p}{p} \lambda}}{\Gamma(r) (\frac{p}{1-p})^r} d\lambda \\
= & \frac{1}{k! \Gamma(r) (\frac{p}{1-p})^r}  \int_0^\infty  \lambda^{k+r-1}  e^{-\frac{\lambda}{p}} d\lambda \\
 = & {k+r-1 \choose k} p^k (1-p)^r
\end{split}\]

\textbf{Homework 1.}   $\sum\limits_{k=0}^\infty Gamma(x | k, \beta) Poisson (k | \lambda)$.

\subsection{Statistical Inference (I)}

\subsubsection{Jeffrey Prior}
In order to show Jeffrey prior, we first introduce \textbf{Fisher information}. In mathematical statistics, the Fisher information is a way of measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $\theta$ upon which the probability of $X$ depends.

Assume we have a model for random variable $X$, for example $\BP(X| \theta)$. $\BP(X | \theta)$ can be seen as a joint function of $x$ and $\theta$. Let $f(x, \theta) = \BP(X| \theta)$. Then Fisher information of $X$ about $\theta$ is given by:
\[\begin{split} I(\theta)  = &  \BE [(\frac{\partial \log f(x, \theta)}{\partial \theta})^2] \\
 = & \int \left( \frac{\partial \log f(x, \theta)}{\partial \theta} \right)^2 f(x,\theta) d\theta
\end{split}\]

\begin{lemma}
  Under certain condition, \[ I(\theta) = -\BE[\frac{\partial^2 \log f}{\partial \theta^2}]\]
\end{lemma}

\textit{Proof}.
  \begin{equation*}
    \begin{split}
      \frac{\partial^2 \log f}{\partial \theta^2} = & \frac{\partial}{\partial \theta} \left( \frac{f'}{f}\right) \\
       = & \frac{f''}{f} - \left(\frac{f'}{f}\right)^2 \\
       = & \frac{f''}{f} - \left(\frac{\partial \log f}{\partial \theta}\right)^2 
    \end{split}
  \end{equation*}
  So, \[\begin{split}
  \BE[\frac{\partial^2 \log f}{\partial \theta^2}] & = \int \frac{\partial^2 \log f}{\partial \theta^2} f dx \\
  & = \int \frac{\partial^2 f}{\partial \theta^2} dx - I(\theta) \\
  & = \frac{\partial^2}{\partial \theta^2} \int f dx - I(\theta)  \\
  & = - I(\theta)
  \end{split}\]
\hfill$\Box$
  
Now let we go to see Jeffrey prior. When we do MAP(maximum a posteriori), we usually meet $\BP(\theta | X) \propto \BP(X | \theta) BP(\theta)$. 
Usually $\BP(X | \theta)$ is easy to get, but $\BP(\theta)$ (prior) needs our hypothesis. How to choose hypothesis? If we set a prior with hyper-parameter, the training process will be difficult. Jeffrey prior tells us how to choose hypothesis:
\[ \BP(\theta) \propto \sqrt{I(\theta)}\]

\textbf{Remark:} Jeffrey prior has a property called \textbf{ invariant under reparameterization}, which means if we replace $\theta$ with $\varphi$, and there is a one to one rejection between $\theta$ and $\varphi$. Then we can get:
\[\begin{split} \BP(\varphi) & = \BP(\theta) \left| \frac{\partial \theta}{\partial \varphi} \right| \\
& \propto \sqrt{ I (\theta) \left(\frac{\partial \theta}{\partial \varphi}\right)^2} \\
& = \sqrt{\BE\left[ \left(\frac{\partial \log f}{\partial \theta}\right)^2\right]\left(\frac{\partial \theta}{\partial \varphi}\right)^2} \\
& = \sqrt{\BE\left[ \left( \frac{\partial \log f}{\partial \varphi} \right)^2 \right]}
\end{split}\]

\begin{example}
$X \sim N(\mu, \sigma^2)$.
\end{example}

\textbf{Case 1:} Fix $\sigma$, the only parameter is $\mu$. So we can get:
\[\begin{split} 
I(\mu) &= \BE\left[ \left( \frac{(x - \mu)^2}{\sigma^2} \right)^2 \right] \\
&= \frac{\BE(x - \mu)^2}{\sigma^4} \\
&= \frac{1}{\sigma^2}
\end{split}\]
So we can get Jeffrey prior $\BP(\mu) \propto \sqrt{I(\mu)} = \frac{1}{\sigma}$. As $\sigma$ is fixed, then $\BP(\mu) \propto 1$.

\textbf{Remark:} Although $\BP(\mu) = 1$ is a improper prior, as $\int_{-\infty}^\infty 1 dx = \infty$, the posteriori is proper. 
The prior is also called \textbf{uninformative prior}.

\textbf{Case 2:} Fix $\mu$, the only parameter is $\sigma$. For convenience, let $\tau = \frac{1}{\sigma^2}$. So $f(x) = \frac{\tau^{\frac{1}{2}}}{\sqrt{2\pi}} e^{-\frac{\tau(x-\mu)^2}{2}}$. Then we can get Fisher information:
\[\begin{split}
I(\tau) &= \BE\left[\left( \frac{\partial \log f}{\partial \tau} \right)\right]  \\
&= \BE\left[ \frac{1}{4} \left( \frac{1}{\tau} - (x-\mu)^2 \right)^2  \right] \\
&= \BE\left[ \frac{1}{4\tau^2} - \frac{(x - \mu)^2}{2\tau} + \frac{(x-\mu)^4}{4} \right] \\
&= \frac{1}{4\tau^2} - \frac{1}{2\tau^2} + \frac{1}{4}\BE(x-\mu)^4 \\
&= \frac{1}{2\tau^2}
\end{split}\]

So Jeffrey prior is $\BP(\tau) \propto \sqrt{I(\tau)}$.
\newline

\textbf{Homework 2:} Compute the following integrals:
\begin{enumerate}
\item $m_0 = \int_{-\infty}^{\infty} \Phi(x) N(x|\mu, \sigma^2) dx$
\item $m_1 = \int_{-\infty}^{\infty} \Phi(x) N(x|\mu, \sigma^2)x dx$
\item $m_2 = \int_{-\infty}^{\infty} \Phi(x) N(x|\mu, \sigma^2)(x-m_1) dx$

where $\Phi(x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} dt$
\end{enumerate}

\begin{example}
$X \sim Poisson(\lambda)$
\end{example}
Fisher information is:
\[\begin{split}
I(\lambda) &= \BE\left[ \left( \frac{n}{\lambda} - 1\right)^2 \right] \\
&= 1 + \frac{\BE(n^2)}{\lambda^2} - 2 \\
&= \frac{\lambda + 1}{\lambda} - 1 \\
&= \frac{1}{\lambda}
\end{split}\]
So Jeffrey prior is:
\[\begin{split}
\BP(\lambda) \propto \sqrt{\frac{1}{\lambda}}.
\end{split}\]
 
\textbf{Homework 3.} $f(x, \theta) = \theta^x(1-\theta)^{1-x}$, $0 < \theta < 1$.
\begin{enumerate}
\item Compute Jeffrey prior about $\theta$.
\item If $\theta = \sin^2 \alpha$, compute Jeffrey prior about $\alpha$.
\end{enumerate}
 
\subsubsection{Problem: $X = \theta + \epsilon$}
Assume we have a model $X = \theta + \epsilon$, where $X$ is data which we observed or predict, $\theta$ is the parameters, $\epsilon \sim N(0, \tau)$ is the error. So given $\theta$, $X \sim N(\theta, \tau)$. When we use MAP(maximum a posteriori) to estimate parameter $\theta$, we will get $\BP(\theta | X) \propto \BP(X | \theta)\BP(\theta)$. 
We will discuss this problem under several different conditions in the following.

\textbf{Case 1.} Fix $\tau$, or let it be hyper-parameter. The only parameter is $\theta$. And we set the prior about $\theta$ is $N(\theta | 0, \lambda)$. So
\[\begin{split} 
\BP(\theta | x) &\propto \BP(x | \theta) \BP(\theta) \\
&= \frac{1}{\sqrt{2\pi\tau}} e^{-\frac{(x - \theta)^2}{2\tau}} 
\frac{1}{\sqrt{2\pi\lambda}} e^{-\frac{\theta^2}{2\lambda}} \\
&= \frac{1}{2\pi \sqrt{\tau\lambda}} e^{-\frac{1}{2} [(\frac{1}{\tau} + \frac{1}{\lambda})( \theta - \frac{\lambda x}{\tau + \lambda} )^2 + \frac{x^2}{\tau + \lambda}] }
\end{split}\].
Then we can get the estimate about $\theta$ from MAP, $\hat{\theta} = \frac{\lambda x}{\lambda + \tau}$.

\textbf{Case 2.} Let $\theta$ and $\tau$ both be parameters. In order to get MAP, we can make three hypothesis.

\textbf{case 2.1.} Assume $\theta$ and $\tau$ are independent, then 
$\BP(\theta, \tau) = \BP(\theta)\BP(\tau)$. Let $\theta$'s prior be $\theta \sim N(0, \lambda)$, $\tau$'s prior be $\tau \sim Gamma(\alpha, \beta)$. So
\[\begin{split}
\BP(\theta, \tau | X) &\propto \BP(X | \theta, \tau) \BP(\theta) \BP(\tau) \\
&= \frac{\tau^{-\frac{1}{2}}}{\sqrt{2\pi}} e^{-\frac{(x - \theta)^2}{2\tau}}
\frac{1}{\sqrt{2\pi\lambda}} e^{-\frac{\theta^2}{2\lambda}} 
\frac{\beta^\alpha}{\Gamma(\alpha)} \tau^{\alpha-1} e^{-\beta\tau}
\end{split}\] 

In order to get the maximum, it's equivalent to compute the minimum $\log$. Let $L = \log \BP(X|\theta, \tau)\BP(\theta)\BP(\tau)$, remove the constant, we can get:
\[ L = \beta\tau + \frac{1}{2}\left[ \frac{(x-\theta)^2}{\tau} + \frac{\theta^2}{\lambda} \right] - (\alpha-\frac{3}{2})\log \tau
\]
To get the estimate of $\theta$ and $\tau$, we need to solve:
\[
\begin{cases}
\frac{\partial L}{\partial \theta} = 0 \\
\frac{\partial L}{\partial \tau} = 0
\end{cases}
\]
Then we will get:
\[\begin{cases}
(\frac{1}{\lambda} + \frac{1}{\tau})\theta - \frac{x}{\tau} = 0 \\
\beta - \frac{(x-\theta)^2}{2\tau^2} - (\alpha - \frac{3}{2})\frac{1}{\tau} = 0.
\end{cases}\]
It is a difficult problem to solve, especially when $\theta$ is a vector.

\textbf{Remark :} One way to solve the problem above is to compute one parameter, for example $\theta$, when fixing the other parameter, i.e. $\tau$. Then fix $\theta$, compute $\theta$. Hold on until they get convergent. Well, then we need to think about the convergence problem.

\textbf{case 2.2.} Assume the conditional prior of $\theta | \tau$ is $\theta | \tau \sim N(0, \lambda\tau)$, the prior of $\tau$ is $\tau \sim \Gamma(\alpha, \beta)$ as case 2.1. So
\[\begin{split}
\BP(\theta, \tau | X) &\propto \BP(X | \theta, \tau) \BP(\theta | \tau) \BP(\tau) \\
&= \frac{1}{\sqrt{2\pi\tau}} e^{-\frac{(x-\theta)^2}{2\tau}}
\frac{1}{\sqrt{2\pi\lambda \tau}} e^{-\frac{\theta^2}{2\lambda\tau}}
\frac{\beta^\alpha}{\Gamma(\alpha)} \tau^{\alpha - 1} e^{-\beta\tau}
\end{split}
\]
Then the corresponding $L$ is given by:
\[ L = \beta\tau - (a-2)\ln \tau + \frac{(x-\theta)^2}{2\tau} + \frac{\theta^2}{2\lambda \tau}
\].
To get the estimate of $\theta$ and $\tau$, we need to solve:
\[\begin{cases}
\frac{\partial L}{\partial \theta} = 0 \\
\frac{\partial L}{\partial \tau} = 0
\end{cases}
\]
Then we will get:
\begin{equation*} \begin{cases}
\frac{1}{\tau}(\frac{\theta}{\lambda} + \theta - x) = 0 \hfill (1) \\
\beta - \frac{\alpha - 2}{\tau} - \frac{1}{\tau^2}\left(\frac{(x-\theta)^2}{2} + \frac{\theta^2}{2\lambda} \right) = 0 \hfill(2)
\end{cases}
\end{equation*}
Form (1), we can easily get $\theta$. It is called \textbf{decouple}.

\textbf{case 2.3.} From the two subcases above, we can find the major problem is computing complexity. Another problem will occurs if there are too many hyper-parameters. As we need to search the best hyper-parameters in grids. So if there are 2 hyper-parameters, the search space is 2-dimension. If there are 3 hyper-parameters, the search space is 3-dimension... It will cost much time when the search space is high dimension.

Simply, we can give an uninformative prior to $\tau$, $\BP(\tau) \propto 1$. Or we can consider Jeffrey prior for $\tau$. According to $\theta | \tau \sim N(0, \lambda\tau)$. Then we can get Fisher information:
\[\begin{split}
I(\tau) &= \BE[ (\frac{\partial \ln f}{\partial \tau}) ]  \\
&= \frac{1}{2\tau^2}
\end{split}\],
where $f = \frac{1}{\sqrt{2\pi\lambda\tau}} e^{-\frac{\theta^2}{2\lambda\tau}}$. So we can get the prior for $\tau$, $\BP(\tau) \propto \frac{1}{\tau}$. Then we will get:
\[\begin{split}
\BP(\theta, \tau | x) &\propto \BP(x | \theta, \tau) \BP(\theta | \tau) \BP(\tau) \\
&= \frac{1}{\sqrt{2\pi\tau}} e^{-\frac{(x-\theta)^2}{2\tau}} \frac{1}{\sqrt{2\pi\lambda\tau}} e^{-\frac{\theta^2}{2\lambda\tau}} \frac{1}{\tau} \\
\end{split}\]
After $-\ln$ operation and remove constants, we will get:
\[ L = 2\ln\tau + \frac{(x-\theta)^2}{2\tau} + \frac{\theta^2}{2\lambda\tau} \].
Then accoring to $\frac{\partial L}{\partial \theta} = 0$ and $\frac{\partial L}{\partial \tau} = 0$, we will get the followings:
\[\begin{cases}
\frac{1}{\tau} \left[\frac{\theta}{\lambda} - \theta\right] = 0 \\
\frac{2}{\tau} - \frac{1}{\tau^2}\left(\frac{(x-\theta)^2}{2} + \frac{\theta^2}{2\lambda}\right) = 0
\end{cases}\].
We can see it is easy to solve.

\end{document}




