\documentclass[11pt]{article}
\usepackage{graphicx} % more modern
%\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{epsf}
\usepackage{amsmath,amssymb,amsfonts,verbatim}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
%\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}

\def\A{{\bf A}}
\def\a{{\bf a}}
\def\B{{\bf B}}
\def\b{{\bf b}}
\def\C{{\bf C}}
\def\c{{\bf c}}
\def\D{{\bf D}}
\def\d{{\bf d}}
\def\E{{\bf E}}
\def\e{{\bf e}}
\def\F{{\bf F}}
\def\f{{\bf f}}
\def\G{{\bf G}}
\def\g{{\bf g}}
\def\k{{\bf k}}
\def\K{{\bf K}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\L{{\bf L}}
\def\M{{\bf M}}
\def\m{{\bf m}}
\def\n{{\bf n}}
\def\N{{\bf N}}
\def\BP{{\bf P}}
\def\R{{\bf R}}
\def\BS{{\bf S}}
\def\s{{\bf s}}
\def\t{{\bf t}}
\def\T{{\bf T}}
\def\U{{\bf U}}
\def\u{{\bf u}}
\def\V{{\bf V}}
\def\v{{\bf v}}
\def\W{{\bf W}}
\def\w{{\bf w}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\Q{{\bf Q}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\Z{{\bf Z}}
\def\z{{\bf z}}
\def\0{{\bf 0}}
\def\1{{\bf 1}}


\def\hx{\hat{\bf x}}
\def\tx{\tilde{\bf x}}
\def\ty{\tilde{\bf y}}
\def\tz{\tilde{\bf z}}
\def\hd{\hat{d}}
\def\HD{\hat{\bf D}}

\def\MF{{\mathcal F}}
\def\MR{{\mathcal R}}
\def\MG{{\mathcal G}}
\def\MI{{\mathcal I}}
\def\MN{{\mathcal N}}
\def\MO{{\mathcal O}}
\def\MT{{\mathcal T}}
\def\MX{{\mathcal X}}
\def\SW{{\mathcal {SW}}}
\def\MW{{\mathcal W}}
\def\MY{{\mathcal Y}}
\def\BR{{\mathbb R}}
\def\BP{{\mathbb P}}

\def\bet{\mbox{\boldmath$\beta$\unboldmath}}
\def\epsi{\mbox{\boldmath$\epsilon$}}

\def\etal{{\em et al.\/}\,}
\def\tr{\mathrm{tr}}
\def\rk{\mathrm{rk}}
\def\diag{\mathrm{diag}}
\def\dg{\mathrm{dg}}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\vecd{\mathrm{vec}}

\def\ph{\mbox{\boldmath$\phi$\unboldmath}}
\def\vp{\mbox{\boldmath$\varphi$\unboldmath}}
\def\pii{\mbox{\boldmath$\pi$\unboldmath}}
\def\Ph{\mbox{\boldmath$\Phi$\unboldmath}}
\def\pss{\mbox{\boldmath$\psi$\unboldmath}}
\def\Ps{\mbox{\boldmath$\Psi$\unboldmath}}
\def\muu{\mbox{\boldmath$\mu$\unboldmath}}
\def\Si{\mbox{\boldmath$\Sigma$\unboldmath}}
\def\lam{\mbox{\boldmath$\lambda$\unboldmath}}
\def\Lam{\mbox{\boldmath$\Lambda$\unboldmath}}
\def\Gam{\mbox{\boldmath$\Gamma$\unboldmath}}
\def\Oma{\mbox{\boldmath$\Omega$\unboldmath}}
\def\De{\mbox{\boldmath$\Delta$\unboldmath}}
\def\de{\mbox{\boldmath$\delta$\unboldmath}}
\def\Tha{\mbox{\boldmath$\Theta$\unboldmath}}
\def\tha{\mbox{\boldmath$\theta$\unboldmath}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]


\def\probin{\mbox{\rotatebox[origin=c]{90}{$\vDash$}}}

\def\calA{{\cal A}}



%this is a comment

%use this as a template only... you may not need the subsections,
%or lists however they are placed in the document to show you how
%do it if needed.


%THINGS TO REMEMBER
%to compile a latex document - latex filename.tex
%to view the document        - xdvi filename.dvi
%to create a ps document     - dvips filename.dvi
%to create a pdf document    - dvipdf filename.dvi
%{\bf TEXT}                  - bold font TEXT
%{\it TEXT}                  - italic TEXT
%$ ... $                     - places ... in math mode on same line
%$$ ... $$                   - places ... in math mode on new line
%more info at www.cs.wm.edu/~mliskov/cs423_fall04/tex.html


\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\notes}[5]{
	\renewcommand{\thepage}{#1 - \arabic{page}}
	\noindent
	\begin{center}
	\framebox{
		\vbox{
		\hbox to 5.78in { { \bf Statistical Machine Learning}
		\hfill #2}
		\vspace{4mm}
		\hbox to 5.78in { {\Large \hfill #5 \hfill} }
		\vspace{2mm}
		\hbox to 5.78in { {\it #3 \hfill #4} }
		}
	}
	\end{center}
	\vspace*{4mm}
}

\newcommand{\ho}[5]{\notes{#1}{Introduction}{Professor: Zhihua Zhang}{}{Lecture Notes #1: Introduction}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%begins a LaTeX document
\begin{document}

\ho{0}{2011.02.21}{Moses Liskov}{Name}{Lecture title}
What's statistical machine learning? Here is a quote from Jordan, ``A field that bridges computation and statistics with ties to information theory, signal processing, algorithms, control theory and optimization theory."

In machine learning, data is typically expressed in a matrix form. Suppose we have $n$ samples, $p$ variables (or features). Then we have
\[X = \left(
       \begin{array}{cccc}
         x_{11} & x_{12} & \cdots & x_{1p} \\
         x_{21} & x_{22} & \cdots & x_{2p} \\
         \vdots &  &  & \vdots \\
         x_{n1} & x_{n2} & \cdots & x_{np} \\
       \end{array}
     \right)_{n \times p}
\]
The $i$th sample can be denoted as $X_i = (X_{11}, X_{12}, \dots, X_{1p})^T$.
        
Machine learning is mainly to solve the following problems:
\begin{enumerate}[(1)]
\item 
\textbf{Dimension Reduction:}
Dimension reduction is the process of reducing the number of random variables(or features) under consideration. Formally, let $X_i \in \BR^p$, we want to find $Z_i \in \BR^q (q < p)$ to present $X_i$.

If we use linear transformation, then we need to find a matrix $A$ such that $Z_i = AX_i$. Note that $A$ should be full row rank.

If we use nonlinear transformation, then we need to find a nonlinear function $f$ such that $Z_i = f(X_i)$.
\item 
\textbf{Clustering:}
Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).We can view $n$ samples as $n$ points, and our object is to cluster them into $k$ clusters.
\item 
\textbf{Classification:}
Classification is the problem of identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Formally, in the training set, we have a label $Y_i$ for each $X_i$, where $Y_i \in C$, $C$ is a non-empty finite set. If $Y_i \in \{-1, 1\}$ or $\{0, 1\}$, it's a binary classification problem. If $Y_i \in \{1, 2, \dots, k\}$, it's a multi-class classification problem. There are also problems that one observation belongs to more than one category and they are called multi-label or multi-output classification.
\item 
\textbf{Regression:}
Regression is a particular classification problem in which the label $Y_i \in \BR$.
\item 
\textbf{Ranking:}
also called isotonic regression(IR). Isotonic regression involves finding a weighted least-squares fit $x \in \BR^n $ to a vector $a \in \BR^n$ with weights vector $w \in \BR^n$ subject to a set of non-contradictory constraints of kind $x_i \geq x_j$.
\end{enumerate}

Note that (1),(2) are unsupervised learning, (3),(4),(5) are supervised learning. Unsupervised learning is that of trying to find hidden structure in unlabeled data. Supervised learning is the machine learning task of inferring a function from labelled training data. 

For supervised learning, the data is usually split into two or three parts.
\begin{enumerate}[(1)]
\item \textbf{Training data:} A set of examples used for learning, that is to fit the parameters (e.g., weights for neural networks) of the model. 
\item \textbf{Validation data:}  Sometimes, we also need a validation set to tune the model, for example to choose the number of hidden units in a neural network or for pruning a decision tree. It is usually used to prevent overfitting and enhance the generalization ability.
\item \textbf{Test data:} This data set is used only for testing the final solution in order to confirm the actual performance.  
\end{enumerate}

\section{Frequentist's view vs. Bayesian view}
\subsection{Frequentist's view}
The frequentistic approach views the model parameters as unknown constants and estimates them by matching the model to the training data using an appropriate metric.

\begin{example}
Suppose we have n pairs of samples $\{(x_i, y_i)\}_{i=1}^{n}$, $x_i \in \BR^p$, $y_i \in \BR$ and we want to fit a linear function $x_i^Ta$(More strictly, it should be $x_i^T a+b$ or include a constant variable 1 in $x_i$) to predict $y_j$.

Using least squares, we have loss function $L = \sum_{i=1}^n (y_i - x_i^T a)^2$, where $a$ is an unknown fixed parameter. We can solve $a$ by minimizing the loss function.

Using maximum likelihood estimation, let $y_i \sim \MN(x_i^T a, \sigma^2)$, namely, $$p(x_i,y_i)=\frac{1}{(2\pi)^{\frac{1}{2}}\sigma}e^{-\frac{(y_i-x_i^Ta)^2}{2\sigma^2}}.$$ So the log likelihood is (assuming the samples are independent) $$l=log \prod_{i=1}^{n}p(x_i,y_i).$$We can solve $a$ by maximizing the joint likelihood.

Under the above conditions, you can prove that maximum likelihood estimation is the same as least squares.
\end{example}

\subsection{Bayesian view}
The Bayesian approach views the model parameters as a random variable and  estimates them by using Bayes' theorem.

\begin{example}
Let's continue example 1.1, let $y_i \sim \MN(x_i^T a, \sigma^2)$ again. Here $a$ and $\sigma$ are random variables, not constants. Let $a \sim \MN(0, \lambda^2)$, $\sigma^2 \sim \Gamma(\alpha, \beta)$.
Our interest is the posterior probability $P(a\mid x_i, y_i) \propto P(x_i, y_i\mid a)P(a)$. We can use maximum posterior estimation or bayesian estimation to solve $a$.
\end{example}


\section{Parametrics vs. Nonparametrics}
In a parametrical model, the number of parameters is fixed once and for all, independent to the number of the training data.
In a nonparametrical model, the number of parameters can change according to the number of training data.

\begin{example}
In \textbf{Nearest Neighbor} method, the number of parameters is the number of training samples. So this model is nonparametrical model.

In \textbf{Logistic Regression}, the number of parameters is the dimension of the training samples. So this model is parametrical model.
\end{example}

\end{document}




